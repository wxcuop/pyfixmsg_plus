#!/usr/bin/env python3
"""
Comprehensive test runner for PyFixMsg Plus testing framework.
Provides organized test execution with reporting and metrics.
"""
import sys
import os
import subprocess
import argparse
import time
import json
from pathlib import Path
from typing import Dict, List, Optional


class TestRunner:
    """Main test runner for PyFixMsg Plus test suite."""
    
    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.test_results = {}
        self.start_time = None
        self.end_time = None
        
        # Test categories
        self.test_categories = {
            'unit': {
                'description': 'Unit tests for individual components',
                'marker': 'unit',
                'timeout': 300,  # 5 minutes
                'paths': ['tests/unit/test_simple_components.py']
            },
            'integration': {
                'description': 'Integration tests for end-to-end scenarios',
                'marker': 'integration',
                'timeout': 600,  # 10 minutes
                'paths': ['tests/integration/']
            },
            'performance': {
                'description': 'Performance and throughput tests',
                'marker': 'performance',
                'timeout': 1800,  # 30 minutes
                'paths': ['tests/performance/']
            },
            'chaos': {
                'description': 'Chaos engineering tests',
                'marker': 'chaos',
                'timeout': 900,  # 15 minutes
                'paths': ['tests/chaos/']
            },
            'property': {
                'description': 'Property-based tests using Hypothesis',
                'marker': 'property',
                'timeout': 600,  # 10 minutes
                'paths': ['tests/property/']
            }
        }
    
    def run_category(self, category: str, extra_args: List[str] = None) -> Dict:
        """Run tests for a specific category."""
        if category not in self.test_categories:
            raise ValueError(f"Unknown test category: {category}")
        
        config = self.test_categories[category]
        extra_args = extra_args or []
        
        print(f"\n{'='*80}")
        print(f"Running {category.upper()} TESTS")
        print(f"Description: {config['description']}")
        print(f"{'='*80}")
        
        # Build pytest command
        cmd = [
            'python', '-m', 'pytest',
            '-v',
            '--tb=short',
            f'--timeout={config["timeout"]}',
            f'-m', config['marker'],
            '--cov=pyfixmsg_plus',
            '--cov-report=term-missing',
            '--cov-append',
        ]
        
        # Add test paths
        cmd.extend(config['paths'])
        
        # Add extra arguments
        cmd.extend(extra_args)
        
        if self.verbose:
            cmd.append('-s')
        
        # Run tests
        start_time = time.time()
        
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=config['timeout']
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            test_result = {
                'category': category,
                'success': result.returncode == 0,
                'duration': duration,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'command': ' '.join(cmd)
            }
            
            # Parse test results from output
            self._parse_test_output(test_result)
            
            print(f"\n{category.upper()} TESTS COMPLETED")
            print(f"Status: {'PASSED' if test_result['success'] else 'FAILED'}")
            print(f"Duration: {duration:.2f} seconds")
            print(f"Tests run: {test_result.get('tests_run', 'unknown')}")
            print(f"Failures: {test_result.get('failures', 'unknown')}")
            print(f"Errors: {test_result.get('errors', 'unknown')}")
            
            if not test_result['success'] and self.verbose:
                print(f"\nSTDOUT:\n{result.stdout}")
                print(f"\nSTDERR:\n{result.stderr}")
            
            return test_result
            
        except subprocess.TimeoutExpired:
            print(f"\n{category.upper()} TESTS TIMED OUT after {config['timeout']} seconds")
            return {
                'category': category,
                'success': False,
                'duration': config['timeout'],
                'error': 'timeout',
                'command': ' '.join(cmd)
            }
        
        except Exception as e:
            print(f"\n{category.upper()} TESTS FAILED with exception: {e}")
            return {
                'category': category,
                'success': False,
                'duration': 0,
                'error': str(e),
                'command': ' '.join(cmd)
            }
    
    def _parse_test_output(self, result: Dict):
        """Parse pytest output to extract test statistics."""
        output = result.get('stdout', '')
        
        # Look for pytest summary line
        lines = output.split('\n')
        for line in lines:
            if '===' in line and ('passed' in line or 'failed' in line):
                # Parse summary line like "=== 5 passed, 2 failed in 10.5s ==="
                parts = line.split()
                
                result['tests_run'] = 0
                result['passed'] = 0
                result['failed'] = 0
                result['errors'] = 0
                result['skipped'] = 0
                
                for i, part in enumerate(parts):
                    if part.isdigit() and i + 1 < len(parts):
                        count = int(part)
                        status = parts[i + 1]
                        
                        if 'passed' in status:
                            result['passed'] = count
                            result['tests_run'] += count
                        elif 'failed' in status:
                            result['failed'] = count
                            result['tests_run'] += count
                        elif 'error' in status:
                            result['errors'] = count
                            result['tests_run'] += count
                        elif 'skipped' in status:
                            result['skipped'] = count
                
                break
    
    def run_all(self, categories: List[str] = None, extra_args: List[str] = None) -> Dict:
        """Run all test categories or specified categories."""
        categories = categories or list(self.test_categories.keys())
        
        print(f"\n{'='*80}")
        print(f"PYFIXMSG PLUS COMPREHENSIVE TEST SUITE")
        print(f"{'='*80}")
        print(f"Categories to run: {', '.join(categories)}")
        print(f"Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        self.start_time = time.time()
        
        # Run each category
        for category in categories:
            try:
                result = self.run_category(category, extra_args)
                self.test_results[category] = result
            except KeyboardInterrupt:
                print(f"\n\nTest execution interrupted by user")
                break
            except Exception as e:
                print(f"\n\nUnexpected error running {category} tests: {e}")
                self.test_results[category] = {
                    'category': category,
                    'success': False,
                    'error': str(e)
                }
        
        self.end_time = time.time()
        
        # Generate summary
        self._generate_summary()
        
        return self.test_results
    
    def _generate_summary(self):
        """Generate and display test summary."""
        total_duration = self.end_time - self.start_time
        
        print(f"\n{'='*80}")
        print(f"TEST EXECUTION SUMMARY")
        print(f"{'='*80}")
        print(f"Total duration: {total_duration:.2f} seconds")
        print(f"End time: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Category results
        print(f"\nCategory Results:")
        print(f"{'Category':<15} {'Status':<10} {'Duration':<12} {'Tests':<8} {'Pass':<6} {'Fail':<6}")
        print(f"{'-'*70}")
        
        total_tests = 0
        total_passed = 0
        total_failed = 0
        overall_success = True
        
        for category, result in self.test_results.items():
            status = "PASSED" if result.get('success', False) else "FAILED"
            duration = f"{result.get('duration', 0):.1f}s"
            tests_run = result.get('tests_run', '?')
            passed = result.get('passed', '?')
            failed = result.get('failed', '?')
            
            print(f"{category:<15} {status:<10} {duration:<12} {tests_run:<8} {passed:<6} {failed:<6}")
            
            if result.get('success', False):
                if isinstance(tests_run, int):
                    total_tests += tests_run
                if isinstance(passed, int):
                    total_passed += passed
                if isinstance(failed, int):
                    total_failed += failed
            else:
                overall_success = False
        
        print(f"{'-'*70}")
        print(f"{'TOTAL':<15} {'PASSED' if overall_success else 'FAILED':<10} {total_duration:.1f}s{'':<6} {total_tests:<8} {total_passed:<6} {total_failed:<6}")
        
        # Success rate
        if total_tests > 0:
            success_rate = (total_passed / total_tests) * 100
            print(f"\nOverall success rate: {success_rate:.1f}%")
        
        # Failed categories
        failed_categories = [cat for cat, result in self.test_results.items() if not result.get('success', False)]
        if failed_categories:
            print(f"\nFailed categories: {', '.join(failed_categories)}")
        
        return overall_success
    
    def run_smoke_test(self):
        """Run a quick smoke test to verify basic functionality."""
        print("\n" + "="*80)
        print("RUNNING QUICK SMOKE TEST")
        print("="*80)
        
        # Run just the basic working tests
        test_files = [
            "tests/test_configmanager.py",
            "tests/test_databasemessagestore.py", 
            "tests/unit/test_simple_components.py"
        ]
        
        for test_file in test_files:
            if not os.path.exists(test_file):
                print(f"Warning: {test_file} not found, skipping...")
                continue
        
        cmd = ["python", "-m", "pytest"] + test_files + ["-v", "--tb=short"]
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            print("Smoke test PASSED")
            return True
        else:
            print("Smoke test FAILED")
            if self.verbose:
                print("STDOUT:")
                print(result.stdout)
                print("STDERR:")
                print(result.stderr)
            return False
    
    def generate_report(self, output_file: str = None):
        """Generate detailed test report."""
        if not self.test_results:
            print("No test results to report")
            return
        
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_duration': self.end_time - self.start_time if self.end_time and self.start_time else 0,
            'categories': self.test_results,
            'summary': {
                'total_categories': len(self.test_results),
                'passed_categories': len([r for r in self.test_results.values() if r.get('success', False)]),
                'failed_categories': len([r for r in self.test_results.values() if not r.get('success', False)])
            }
        }
        
        if output_file:
            with open(output_file, 'w') as f:
                json.dump(report, f, indent=2)
            print(f"Detailed report saved to: {output_file}")
        
        return report


def main():
    """Main entry point for test runner."""
    parser = argparse.ArgumentParser(
        description="PyFixMsg Plus Test Runner",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python test_runner.py --all                    # Run all tests
  python test_runner.py --unit --integration     # Run specific categories
  python test_runner.py --smoke                  # Quick smoke test
  python test_runner.py --performance --verbose  # Run performance tests with verbose output
        """
    )
    
    # Test category options
    parser.add_argument('--all', action='store_true', help='Run all test categories')
    parser.add_argument('--unit', action='store_true', help='Run unit tests')
    parser.add_argument('--integration', action='store_true', help='Run integration tests')
    parser.add_argument('--performance', action='store_true', help='Run performance tests')
    parser.add_argument('--chaos', action='store_true', help='Run chaos engineering tests')
    parser.add_argument('--property', action='store_true', help='Run property-based tests')
    parser.add_argument('--smoke', action='store_true', help='Run quick smoke test only')
    
    # Options
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')
    parser.add_argument('--report', help='Generate detailed report to file')
    parser.add_argument('--parallel', action='store_true', help='Run tests in parallel (experimental)')
    
    # Additional pytest arguments
    parser.add_argument('pytest_args', nargs='*', help='Additional arguments to pass to pytest')
    
    args = parser.parse_args()
    
    # Determine which categories to run
    categories = []
    if args.all:
        categories = ['unit', 'integration', 'performance', 'chaos', 'property']
    else:
        if args.unit:
            categories.append('unit')
        if args.integration:
            categories.append('integration')
        if args.performance:
            categories.append('performance')
        if args.chaos:
            categories.append('chaos')
        if args.property:
            categories.append('property')
    
    # Default to unit tests if nothing specified
    if not categories and not args.smoke:
        categories = ['unit']
    
    # Create test runner
    runner = TestRunner(verbose=args.verbose)
    
    try:
        if args.smoke:
            # Run smoke test
            success = runner.run_smoke_test()
            sys.exit(0 if success else 1)
        else:
            # Run full test suite
            results = runner.run_all(categories, args.pytest_args)
            
            # Generate report if requested
            if args.report:
                runner.generate_report(args.report)
            
            # Exit with appropriate code
            overall_success = all(result.get('success', False) for result in results.values())
            sys.exit(0 if overall_success else 1)
    
    except KeyboardInterrupt:
        print("\n\nTest execution interrupted by user")
        sys.exit(130)  # Standard exit code for Ctrl+C
    
    except Exception as e:
        print(f"\nUnexpected error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
